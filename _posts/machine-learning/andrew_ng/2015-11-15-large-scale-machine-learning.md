---
layout: page
title: Large Scale Machine Learning
tagline:
categories: machine-learning

---

Batch Gradient Descent

- classic way to do gradient descent

Stochastic Gradient Descent

1. Randomly shuffle dataset
2. Repeat for {i := 1,...,m} update the $$\theta$$

Mini-Batch Gradient Descent

1. Randomly shuffle dataset
2. Repeat for {i := 1,11,21,..,m}, every time use b items to update the $$\theta$$

Stochastic Gradient Descent Convergence

$$
\alpha = \frac{const1}{iterationNumner + const2}
$$

Online Learning

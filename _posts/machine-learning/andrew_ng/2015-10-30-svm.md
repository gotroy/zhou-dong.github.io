---
layout: page
title: Support Vector Machines
tagline:
categories: machine-learning

---

$$
\min_{\theta} = C \sum_{i=1}^m \biggr[ y^{(i)} \operatorname{cost}_1 (\theta^Tx^{(i)}) + (1-y^{(i)})\operatorname{cost}_0 (\theta^Tx^{(i)}) \biggr] + \frac{1}{2} \sum_{j=1}^n \theta_j^2
$$

---

#### 要了解支撑向量机，首先要弄明白：向量相乘的几何意义。

- 向量u的模 $$\times$$ 向量v在向量u上的投影。
- $$u \times v = \| u \| \cdot \| v \| \cos(\theta)$$。
- 向量相乘，可以表示`一个向量在另一个向量方向上投影的距离`。

如果一个向量空间中有两个向量：一个正向量，一个负向量。

---

#### 从不同的角度解释“向量相乘”

$$
u =
\begin{bmatrix}
u_0 \\
u_1
\end{bmatrix}

\ \ \ \

v =
\begin{bmatrix}
v_0 \\
v_1
\end{bmatrix} \\

u^Tv =
\begin{bmatrix}
u_0 & u_1
\end{bmatrix}

\begin{bmatrix}
v_0 \\
v_1
\end{bmatrix}

= u_0 v_0 + u_1 v_1 \\
$$

<img height="200px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/2000px-Dot_Product.svg.png" />

$$
\parallel u \parallel = \sqrt{(u_0^2 + u_1^2)} \\
p = \text{length of projection of v onto u} \\
u^Tv = p \times \parallel u \parallel \\
$$

<img src="http://cdn1.askiitians.com/Images/2014108-144310695-8606-dot-product-image.PNG" />


$$
u \times v = \parallel u \parallel \parallel v \parallel \cos(\theta)
$$

---

#### Reference：

https://www.youtube.com/watch?v=_PwhiWxHK8o

---
layout: page
title: Support Vector Machines
tagline:
categories: machine-learning

---

$$
\min_{\theta} = C \sum_{i=1}^m \biggr[ y^{(i)} \operatorname{cost}_1 (\theta^Tx^{(i)}) + (1-y^{(i)})\operatorname{cost}_0 (\theta^Tx^{(i)}) \biggr] + \frac{1}{2} \sum_{j=1}^n \theta_j^2
$$

---

#### 要了解支撑向量机，首先要弄明白：向量相乘的几何意义。

- 向量u的模 $$\times$$ 向量v在向量u上的投影。
- $$u \times v = \| u \| \cdot \| v \| \cos(\theta)$$。

- 向量相乘，可以表示`一个向量在另一个向量方向上投影的距离`。


#### 支撑向量机的推理过程：

- 如果一个向量空间中有两个向量：一个正向量u，一个负向量v，一个分割平面c。

$$
w \cdot u \geqslant c \\
w \cdot v \leqslant c
$$

- 如果要在间隔的话，即：margin

$$
w \cdot u - b \geqslant c \\
w \cdot v + b \leqslant c
$$

- 如果把c平面看做0平面的话，即：平面上面的大于等于0，下面的小于等于0

$$
w \cdot u - b \geqslant 0 \ \ \ \text{positive} \\
w \cdot v + b \leqslant 0 \ \ \ \text{negative}
$$

- 如果是间隔为1，并且normalization公式的话：

$$
w \cdot x_+ - 1 \geqslant 0 \ \ \ \text{positive} \\
w \cdot x_- + 1 \leqslant 0 \ \ \ \text{negative} \\
\\
w \cdot x_+ \geqslant 1 \ \ \ \text{positive} \\
w \cdot x_- \leqslant -1 \ \ \ \text{negative}
$$

- 引入一个y，到上面的式子中

$$
y =
\begin{cases}
+1 \ \ \ \text{positive sample}\\
-1 \ \ \ \text{negative sample}
\end{cases}
$$

$$
w \cdot x_+ \geqslant 1 \ \ \ \text{positive} \\
w \cdot x_- \leqslant -1 \ \ \ \text{negative}
$$

$$
y_+ w \cdot x_+ \geqslant 1 \ \ \ \text{positive} \\
y_- w \cdot x_- \geqslant 1 \ \ \ \text{negative}
$$

$$
y \cdot w \cdot x \geqslant 1
$$

$$
y w x -1 \geqslant 0
$$

- 我们可以加上一个阀值：

$$
y (w x + b)  -1 \geqslant 0
$$

- `找出支撑点来`，即在支撑线上的点

$$
y (w x + b)  -1 = 0
$$

- 求支撑线之间的宽度：在+支撑线上的$$x_+$$，与在-支撑线上的$$x_-$$

$$
\text{WIDTH} =  (x_+ - x_-) \cdot \frac{w}{\| w \|}
$$

$$
x_+ = \frac{1-b}{w} \\
x_- = -\frac{1+b}{w} \\
x_+ - x_- = \frac{1-b-(-1-b)}{w} = \frac{2}{w} \\
\text{WIDTH} =  (x_+ - x_-) \cdot \frac{w}{\| w \|} = \frac{2}{w} \cdot \frac{w}{\| w \|}  = \frac{2}{\| w \|} \\
w = \operatorname{arg} \max(\frac{2}{\| w \|}) \\
= \operatorname{arg} \max(\frac{1}{\| w \|}) \\
= \operatorname{arg} \min(\| w \|) \\
= \operatorname{arg} \min(\frac{1}{2}\| w \|^2)
$$

#### 得到支撑向量机的公式表示：

$$
y =
\begin{cases}
+1 \ \ \ \text{positive sample}\\
-1 \ \ \ \text{negative sample}
\end{cases}
\ \ \ (1) \\

y (w x + b)  -1 \geqslant 0
\ \ \ (2) \\

w = \operatorname{arg} \min(\frac{1}{2}\| w \|^2)
\ \ \ (3)
$$

#### 使用“拉格朗日”函数，来求解支撑向量机（Lagrange equation）

$$
L = \frac{1}{2}\| w \|^2 - \sum \alpha_i \Bigl[y_i(wx_i + b) - 1 \Bigl] \\
\frac{\partial L}{\partial w} = w - \sum \alpha_i y_i x_i = 0\\
w = \sum_i \alpha_i y_i x_i \\
\frac{\partial L}{\partial b} = - \sum_i \alpha_i y_i = 0 \\
\sum_i \alpha_i y_i = 0
$$

$$
L = \frac{1}{2} w^Tw - \sum_i \alpha_i \Bigl[y_i(w^Tx_i + b) - 1 \Bigl] \\
L = \frac{1}{2} w^Tw - \sum_i \alpha_i y_i x_i w^T - \sum_i \alpha_i y_i b + \sum_i \alpha_i\\
L = \frac{1}{2} w^Tw - ww^T - 0 + \sum_i \alpha_i\\
L = \sum_i \alpha_i - \frac{1}{2} w^Tw \\
L = \sum_i \alpha_i - \sum_i \sum_j \alpha_i y_i x_i  \alpha_j y_j x_j \\
$$

---

#### 从不同的角度解释“向量相乘”

$$
u =
\begin{bmatrix}
u_0 \\
u_1
\end{bmatrix}

\ \ \ \

v =
\begin{bmatrix}
v_0 \\
v_1
\end{bmatrix} \\

u^Tv =
\begin{bmatrix}
u_0 & u_1
\end{bmatrix}

\begin{bmatrix}
v_0 \\
v_1
\end{bmatrix}

= u_0 v_0 + u_1 v_1 \\
$$

<img height="200px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/2000px-Dot_Product.svg.png" />

$$
\parallel u \parallel = \sqrt{(u_0^2 + u_1^2)} \\
p = \text{length of projection of v onto u} \\
u^Tv = p \times \parallel u \parallel \\
$$

<img src="http://cdn1.askiitians.com/Images/2014108-144310695-8606-dot-product-image.PNG" />


$$
u \times v = \parallel u \parallel \parallel v \parallel \cos(\theta)
$$

---

#### Reference：

https://www.youtube.com/watch?v=_PwhiWxHK8o

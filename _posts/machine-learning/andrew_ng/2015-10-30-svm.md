---
layout: page
title: Support Vector Machines
tagline:
categories: machine-learning

---

$$
\min_{\theta} = C \sum_{i=1}^m \biggr[ y^{(i)} \operatorname{cost}_1 (\theta^Tx^{(i)}) + (1-y^{(i)})\operatorname{cost}_0 (\theta^Tx^{(i)}) \biggr] + \frac{1}{2} \sum_{j=1}^n \theta_j^2
$$

---

#### 从不同的角度解释“向量相乘”

$$
u =
\begin{bmatrix}
u_0 \\
u_1
\end{bmatrix}

\ \ \ \

v =
\begin{bmatrix}
v_0 \\
v_1
\end{bmatrix} \\

u^Tv =
\begin{bmatrix}
u_0 & u_1
\end{bmatrix}

\begin{bmatrix}
v_0 \\
v_1
\end{bmatrix}

= u_0 v_0 + u_1 v_1 \\
$$

<img height="200px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/2000px-Dot_Product.svg.png" />

$$
\parallel u \parallel = \sqrt{(u_0^2 + u_1^2)} \\
p = \text{length of projection of v onto u} \\
u^Tv = p \times \parallel u \parallel \\
$$

<img src="http://cdn1.askiitians.com/Images/2014108-144310695-8606-dot-product-image.PNG" />


$$
u \times v = \parallel u \parallel \parallel v \parallel \cos(\theta)
$$
